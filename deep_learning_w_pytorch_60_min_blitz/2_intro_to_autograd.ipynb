{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to `torch.autograd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "Neural networks have two broad steps:\n",
    "1. Forward propagation: the NN runs to input data and generates guesses based on current weights and biases  \n",
    "  \n",
    "2. Backward propagation: the NN adjusts its parameters proportionate to the error in its guess by traversing backwards from the output and collecting derivatives of the error w.r.t. the weights and biases (called gradients) and optimizing the parameters using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage in PyTorch\n",
    "\n",
    "- Looking at a single training step\n",
    "- Load a pretrained model from `torchvision`\n",
    "- Create a random data tensor to represent a single image with 3 channels, and height/width of 64\n",
    "- Corresponding label is initialized to random values (shape (1,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Pass\n",
    "\n",
    "- Use model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Back Propagation\n",
    "\n",
    "- User the model's prediction to calculate error\n",
    "  \n",
    "- Back propagation is triggered when we call `.backward()` on the error tensor\n",
    "  \n",
    "- Use `autograd` to calculate and store the gradients for each model parameter in the parameter's `.grad` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load an optimizer\n",
    "\n",
    "- Load an optimizer\n",
    "  \n",
    "- Call `.step()` to initiate gradient descent\n",
    "  \n",
    "- Optimizer adjusts each parameter by its gradient stored in `.grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "optim.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
